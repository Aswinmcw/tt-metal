# SPDX-FileCopyrightText: Â© 2023 Tenstorrent Inc.

# SPDX-License-Identifier: Apache-2.0

from loguru import logger

import pytest
import torch

import tt_lib as ttl

from tt_lib.utils import (
    pad_weight,
    tilize_to_list,
    untilize,
    is_close,
)
from tests.tt_eager.python_api_testing.sweep_tests import (
    pytorch_ops,
    tt_lib_ops,
)
from tests.tt_eager.python_api_testing.sweep_tests.comparison_funcs import (
    comp_equal,
    comp_pcc,
)
from models.utility_functions import is_wormhole_b0, skip_for_wormhole_b0
from models.utility_functions import torch2tt_tensor, tt2torch_tensor, pad_by_zero


def get_block_subblock_dim(grid_size, M, K, N):
    in0_block_w = K // grid_size[1] // 32  # 16
    in0_block_h = M // grid_size[0] // 32
    out_block_h = M // grid_size[0] // 32
    out_block_w = N // grid_size[1] // 32

    if out_block_w <= 8:
        out_subblock_w = out_block_w
        out_subblock_h = 8 // out_subblock_w
    else:
        out_subblock_h = 1
        out_subblock_w = 8 // out_subblock_h
        while out_block_w % out_subblock_w != 0:
            out_subblock_w = out_block_w // 2

    return in0_block_w, in0_block_h, out_block_h, out_block_w, out_subblock_h, out_subblock_w


@pytest.mark.parametrize(
    "LN_gamma_beta_mem_config",
    (ttl.tensor.MemoryConfig(ttl.tensor.TensorMemoryLayout.INTERLEAVED, ttl.tensor.BufferType.DRAM),),
    ids=[
        "LN_gamma_beta_DRAM",
    ],
)
@pytest.mark.parametrize(
    "in_LN_mem_config",
    (ttl.tensor.MemoryConfig(ttl.tensor.TensorMemoryLayout.INTERLEAVED, ttl.tensor.BufferType.DRAM),),
    ids=[
        "in_LN_DRAM",
    ],
)
@pytest.mark.parametrize(
    "in_LN_dtype",
    (ttl.tensor.DataType.BFLOAT8_B,),
    ids=["BFLOAT8_B"],
)
def test_bert_sharded_LN_to_LN(in_LN_dtype, in_LN_mem_config, LN_gamma_beta_mem_config, device):
    torch.manual_seed(1234)
    # device shape
    grid_size = (12, 8)
    batch = 12
    # mem config
    interleaved_mem_config_L1 = ttl.tensor.MemoryConfig(
        memory_layout=ttl.tensor.TensorMemoryLayout.INTERLEAVED,
        buffer_type=ttl.tensor.BufferType.L1,
    )
    interleaved_mem_config_DRAM = ttl.tensor.MemoryConfig(
        memory_layout=ttl.tensor.TensorMemoryLayout.INTERLEAVED,
        buffer_type=ttl.tensor.BufferType.DRAM,
    )
    sharded_mem_config = ttl.tensor.MemoryConfig(
        memory_layout=ttl.tensor.TensorMemoryLayout.BLOCK_SHARDED,
        buffer_type=ttl.tensor.BufferType.L1,
    )
    # LN
    in0_shape = (1, 1, batch * 384, 1024)
    M = in0_shape[2]
    K = in0_shape[3]
    LN_in0 = torch.rand(in0_shape) * 2 - 0.95
    LN_in0_t = torch2tt_tensor(LN_in0, device, tt_memory_config=in_LN_mem_config, tt_dtype=in_LN_dtype)
    LN_in0_t_shard = ttl.tensor.interleaved_to_sharded(
        LN_in0_t,
        grid_size,
        [M // grid_size[0], K // grid_size[1]],
        ttl.tensor.TensorMemoryLayout.BLOCK_SHARDED,
        ttl.tensor.ShardOrientation.COL_MAJOR,
    )

    LN_in1 = torch.rand(in0_shape) * 2 - 0.8
    LN_in1_t = torch2tt_tensor(LN_in1, device, tt_memory_config=in_LN_mem_config, tt_dtype=in_LN_dtype)
    LN_in1_t_shard = ttl.tensor.interleaved_to_sharded(
        LN_in1_t,
        grid_size,
        [M // grid_size[0], K // grid_size[1]],
        ttl.tensor.TensorMemoryLayout.BLOCK_SHARDED,
        ttl.tensor.ShardOrientation.COL_MAJOR,
    )
    gamma = torch.rand(1, 1, 32, in0_shape[3]) * 2 - 1
    beta = torch.rand(1, 1, 32, in0_shape[3]) * 2.0 - 1.1
    gamma_t = torch2tt_tensor(
        gamma, device, tt_memory_config=LN_gamma_beta_mem_config, tt_dtype=ttl.tensor.DataType.BFLOAT16
    )
    beta_t = torch2tt_tensor(
        beta, device, tt_memory_config=LN_gamma_beta_mem_config, tt_dtype=ttl.tensor.DataType.BFLOAT16
    )
    epsf = 1e-2

    program_config = ttl.operations.primary.LayerNormShardedMultiCoreProgramConfig(
        compute_with_storage_grid_size=grid_size,
        subblock_w=4,
        block_h=batch,
        block_w=4,
        math_fidelity=ttl.tensor.MathFidelity.HiFi4,
        im_data_format=ttl.tensor.DataType.BFLOAT16,
        out_data_format=ttl.tensor.DataType.BFLOAT8_B,
    )

    logger.info("LN")
    LN_out_t = ttl.operations.primary.add_layernorm(
        LN_in0_t_shard,
        LN_in1_t_shard,
        epsf,
        gamma_t,
        beta_t,
        output_mem_config=sharded_mem_config,
        program_config=program_config,
    )
    logger.info("LN_done")

    # FF1 + GELU
    M = 4608
    K = 1024
    N = 4096
    in0_block_w, in0_block_h, out_block_h, out_block_w, out_subblock_h, out_subblock_w = get_block_subblock_dim(
        grid_size, M, K, N
    )

    in0_shape = [1, 1, M, K]
    in1_shape = [1, 1, K, N]
    bias_shape = [1, 1, N]

    # in0 = torch.randn(in0_shape).bfloat16().float()
    # in0_t = torch2tt_tensor(
    #         in0, device, tt_memory_config=interleaved_mem_config_DRAM, tt_dtype=ttl.tensor.DataType.BFLOAT8_B
    #     )
    # LN_out_t = ttl.tensor.interleaved_to_sharded(
    #         in0_t,
    #         grid_size,
    #         [M // grid_size[0], K // grid_size[1]],
    #         ttl.tensor.TensorMemoryLayout.BLOCK_SHARDED,
    #         ttl.tensor.ShardOrientation.COL_MAJOR,
    #     )

    FF1_in1 = torch.randn(in1_shape).bfloat16().float()
    FF1_bias = torch.randn(bias_shape).bfloat16().float()

    FF1_in1_t = torch2tt_tensor(
        FF1_in1, device, tt_memory_config=interleaved_mem_config_DRAM, tt_dtype=ttl.tensor.DataType.BFLOAT8_B
    )
    FF1_bias_t = pad_by_zero(
        FF1_bias, device, tt_memory_config=interleaved_mem_config_DRAM, tt_dtype=ttl.tensor.DataType.BFLOAT8_B
    )[0]

    program_config = ttl.operations.primary.MatmulMultiCoreReuseMultiCastProgramConfig(
        compute_with_storage_grid_size=grid_size,
        in0_block_w=in0_block_w,
        out_subblock_h=out_subblock_h,
        out_subblock_w=out_subblock_w,
        per_core_M=out_block_h,
        per_core_N=out_block_w,
        transpose_mcast=True,
        fused_activation=(ttl.tensor.FusibleActivation.GELU, True),
    )

    logger.info("FF1")
    out_ff1_t = ttl.operations.primary.matmul(
        LN_out_t,
        FF1_in1_t,
        bias=FF1_bias_t,
        program_config=program_config,
        output_mem_config=sharded_mem_config,
        math_fidelity=ttl.tensor.MathFidelity.LoFi,
    )
    logger.info("FF1_done")

    # FF2
    M = 4608
    K = 4096
    N = 1024
    in0_block_w, in0_block_h, out_block_h, out_block_w, out_subblock_h, out_subblock_w = get_block_subblock_dim(
        grid_size, M, K, N
    )

    in0_shape = [1, 1, M, K]
    in1_shape = [1, 1, K, N]
    bias_shape = [1, 1, N]

    # in0 = torch.randn(in0_shape).bfloat16().float()
    # in0_t = torch2tt_tensor(
    #         in0, device, tt_memory_config=interleaved_mem_config_DRAM, tt_dtype=ttl.tensor.DataType.BFLOAT8_B
    #     )
    # out_ff1_t = ttl.tensor.interleaved_to_sharded(
    #         in0_t,
    #         grid_size,
    #         [M // grid_size[0], K // grid_size[1]],
    #         ttl.tensor.TensorMemoryLayout.BLOCK_SHARDED,
    #         ttl.tensor.ShardOrientation.COL_MAJOR,
    #     )

    FF2_in1 = torch.randn(in1_shape).bfloat16().float()
    FF2_bias = torch.randn(bias_shape).bfloat16().float()

    FF2_in1_t = torch2tt_tensor(
        FF2_in1, device, tt_memory_config=interleaved_mem_config_DRAM, tt_dtype=ttl.tensor.DataType.BFLOAT8_B
    )
    FF2_bias_t = pad_by_zero(
        FF2_bias, device, tt_memory_config=interleaved_mem_config_DRAM, tt_dtype=ttl.tensor.DataType.BFLOAT8_B
    )[0]

    program_config = ttl.operations.primary.MatmulMultiCoreReuseMultiCastProgramConfig(
        compute_with_storage_grid_size=grid_size,
        in0_block_w=in0_block_w,
        out_subblock_h=out_subblock_h,
        out_subblock_w=out_subblock_w,
        per_core_M=out_block_h,
        per_core_N=out_block_w,
        transpose_mcast=True,
        fused_activation=None,
    )

    logger.info("FF1")
    out_ff2_t = ttl.operations.primary.matmul(
        out_ff1_t,
        FF2_in1_t,
        bias=FF2_bias_t,
        program_config=program_config,
        output_mem_config=sharded_mem_config,
        math_fidelity=ttl.tensor.MathFidelity.LoFi,
    )
    logger.info("FF1_done")

    # compare results
    # out_ln_t = ttl.tensor.sharded_to_interleaved(LN_out_t, interleaved_mem_config_DRAM)
    # out_ln = tt2torch_tensor(out_ln_t)
    # ref_lnorm = torch.nn.functional.layer_norm(LN_in0 + LN_in1, LN_in0.shape[-1:], gamma[:, :, 0:1, :].flatten(), beta[:, :, 0:1, :].flatten(), epsf)
    # passing, output = comp_pcc(out_ln, ref_lnorm, 0.98)
    # logger.info(output)

    # out_ff1_t = ttl.tensor.sharded_to_interleaved(out_ff1_t, interleaved_mem_config_DRAM)
    # out_ff1 = tt2torch_tensor(out_ff1_t)
    # ref_ff1 = torch.nn.functional.gelu(ref_lnorm @ FF1_in1 + FF1_bias)
    # passing, output = comp_pcc(out_ff1, ref_ff1, 0.99)
    # logger.info(output)

    # out_ff2_t = ttl.tensor.sharded_to_interleaved(out_ff2_t, interleaved_mem_config_DRAM)
    # out_ff2 = tt2torch_tensor(out_ff2_t)
    # ref_ff2 = ref_ff1 @ FF2_in1 + FF2_bias
    # passing, output = comp_pcc(out_ff2, ref_ff2, 0.99)
    # logger.info(output)

    assert True
