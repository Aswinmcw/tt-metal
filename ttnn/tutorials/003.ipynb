{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Initializing device 0\n",
      "\u001b[38;2;000;128;000m                 Device\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Opening device driver\n",
      "\u001b[32m2023-10-31 20:14:47.326\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Detected 4 PCI devices\n",
      "\u001b[32m2023-10-31 20:14:47.348\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Using 1 Hugepages/NumHostMemChannels for TTDevice (pci_interface_id: 3 device_id: 0xfaca revision: 0)\n",
      "\u001b[32m2023-10-31 20:14:47.353\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Using 1 Hugepages/NumHostMemChannels for TTDevice (pci_interface_id: 2 device_id: 0xfaca revision: 0)\n",
      "\u001b[32m2023-10-31 20:14:47.357\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Using 1 Hugepages/NumHostMemChannels for TTDevice (pci_interface_id: 1 device_id: 0xfaca revision: 0)\n",
      "\u001b[32m2023-10-31 20:14:47.367\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Using 1 Hugepages/NumHostMemChannels for TTDevice (pci_interface_id: 0 device_id: 0xfaca revision: 0)\n",
      "\u001b[32m2023-10-31 20:14:47.462\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Disable PCIE DMA\n",
      "\u001b[32m2023-10-31 20:14:47.462\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Disable PCIE DMA\n",
      "\u001b[32m2023-10-31 20:14:47.462\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Disable PCIE DMA\n",
      "\u001b[32m2023-10-31 20:14:47.462\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Disable PCIE DMA\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | AI CLK for device 0 is:   1202 MHz\n",
      "\u001b[38;2;000;128;000m           BuildKernels\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Skip generating erisc binaries for grayskull\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import ttnn\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device_id = 0\n",
    "device = ttnn.open(device_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "sequence_size = 64\n",
    "num_heads = 4\n",
    "head_size = 32\n",
    "hidden_size = num_heads * head_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize activations and weights using torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_hidden_states = torch.randn((batch_size, sequence_size, hidden_size), dtype=torch.bfloat16)\n",
    "\n",
    "torch_attention_mask = torch.zeros((1, 1, 1, sequence_size), dtype=torch.bfloat16)\n",
    "torch_attention_mask[:, :, ::2, :] = -1e9\n",
    "\n",
    "torch_query_weight = torch.randn((hidden_size, hidden_size), dtype=torch.bfloat16)\n",
    "torch_query_bias = torch.randn((hidden_size,), dtype=torch.bfloat16)\n",
    "torch_key_weight = torch.randn((hidden_size, hidden_size), dtype=torch.bfloat16)\n",
    "torch_key_bias = torch.randn((hidden_size,), dtype=torch.bfloat16)\n",
    "torch_value_weight = torch.randn((hidden_size, hidden_size), dtype=torch.bfloat16)\n",
    "torch_value_bias = torch.randn((hidden_size,), dtype=torch.bfloat16)\n",
    "torch_output_weight = torch.randn((hidden_size, hidden_size), dtype=torch.bfloat16)\n",
    "torch_output_bias = torch.randn((hidden_size,), dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert activations and weights to ttnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | from_torch                                         external                                          \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | from_torch                                         external                                          \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | from_torch                                         external                                          \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | from_torch                                         external                                          \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | from_torch                                         external                                          \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | from_torch                                         external                                          \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | from_torch                                         external                                          \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | from_torch                                         external                                          \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | from_torch                                         external                                          \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | from_torch                                         external                                          \n"
     ]
    }
   ],
   "source": [
    "hidden_states = ttnn.from_torch(torch_hidden_states)\n",
    "attention_mask = ttnn.from_torch(torch_attention_mask)\n",
    "query_weight = ttnn.from_torch(torch_query_weight)\n",
    "query_bias = ttnn.from_torch(torch_query_bias)\n",
    "key_weight = ttnn.from_torch(torch_key_weight)\n",
    "key_bias = ttnn.from_torch(torch_key_bias)\n",
    "value_weight = ttnn.from_torch(torch_value_weight)\n",
    "value_bias = ttnn.from_torch(torch_value_bias)\n",
    "output_weight = ttnn.from_torch(torch_output_weight)\n",
    "output_bias = ttnn.from_torch(torch_output_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | to_device                                          external                                          \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | to_device                                          external                                          \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | to_device                                          external                                          \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | to_device                                          external                                          \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | to_device                                          external                                          \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | to_device                                          external                                          \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | to_device                                          external                                          \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | to_device                                          external                                          \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | to_device                                          external                                          \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | to_device                                          external                                          \n"
     ]
    }
   ],
   "source": [
    "hidden_states = ttnn.to_device(hidden_states, device)\n",
    "attention_mask = ttnn.to_device(attention_mask, device)\n",
    "query_weight = ttnn.to_device(query_weight, device)\n",
    "query_bias = ttnn.to_device(query_bias, device, memory_config=ttnn.L1_MEMORY_CONFIG)\n",
    "key_weight = ttnn.to_device(key_weight, device)\n",
    "key_bias = ttnn.to_device(key_bias, device, memory_config=ttnn.L1_MEMORY_CONFIG)\n",
    "value_weight = ttnn.to_device(value_weight, device)\n",
    "value_bias = ttnn.to_device(value_bias, device, memory_config=ttnn.L1_MEMORY_CONFIG)\n",
    "output_weight = ttnn.to_device(output_weight, device)\n",
    "output_bias = ttnn.to_device(output_bias, device, memory_config=ttnn.L1_MEMORY_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write multi_head_attention using ttnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention(\n",
    "    hidden_states,\n",
    "    attention_mask,\n",
    "    query_weight,\n",
    "    query_bias,\n",
    "    key_weight,\n",
    "    key_bias,\n",
    "    value_weight,\n",
    "    value_bias,\n",
    "    output_weight,\n",
    "    output_bias,\n",
    "    *,\n",
    "    head_size,\n",
    "):\n",
    "    batch_size, sequence_size, hidden_size = hidden_states.shape\n",
    "    num_heads = hidden_size // head_size\n",
    "\n",
    "    query = hidden_states @ query_weight\n",
    "    query = query + query_bias\n",
    "    query = ttnn.reshape(query, (batch_size, sequence_size, num_heads, head_size))\n",
    "    query = ttnn.permute(query, (0, 2, 1, 3))\n",
    "\n",
    "    key = hidden_states @ key_weight\n",
    "    key = key + key_bias\n",
    "    key = ttnn.reshape(key, (batch_size, sequence_size, num_heads, head_size))\n",
    "    key = ttnn.permute(key, (0, 2, 3, 1))\n",
    "\n",
    "    value = hidden_states @ value_weight\n",
    "    value = value + value_bias\n",
    "    value = ttnn.reshape(value, (batch_size, sequence_size, num_heads, head_size))\n",
    "    value = ttnn.permute(value, (0, 2, 1, 3))\n",
    "\n",
    "    attention_scores = query @ key\n",
    "    attention_scores = attention_scores * (1 / (head_size**0.5))\n",
    "    if attention_mask is not None:\n",
    "        attention_scores = attention_scores + attention_mask\n",
    "\n",
    "    attention_probs = ttnn.softmax(attention_scores, dim=-1)\n",
    "\n",
    "    context_layer = attention_probs @ value\n",
    "    context_layer = ttnn.permute(context_layer, (0, 2, 1, 3))\n",
    "    context_layer = ttnn.reshape(context_layer, (batch_size, sequence_size, hidden_size))\n",
    "\n",
    "    self_output = context_layer @ output_weight\n",
    "    self_output = self_output + output_bias\n",
    "\n",
    "    return self_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run multi_head_attention using ttnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::Tilize                               device                                            \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::Tilize                               device                                            \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::Matmul                               device                                            \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::TilizeWithValPadding                 device                                            \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::EltwiseBinaryBroadcast               device                                            \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::Untilize                             device                                            \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | from_device                                        external                                          \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | to_torch                                           external                                          \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Tensor.reshape                                     external                                          \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | from_torch                                         external                                          \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | to_device                                          external                                          \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::TilizeWithValPadding                 device                                            \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::Transpose                            device                                            \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::Unpad                                device                                            \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::Tilize                               device                                            \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::Tilize                               device                                            \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::Matmul                               device                                            \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::TilizeWithValPadding                 device                                            \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::EltwiseBinaryBroadcast               device                                            \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::Untilize                             device                                            \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | from_device                                        external                                          \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | to_torch                                           external                                          \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Tensor.reshape                                     external                                          \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | from_torch                                         external                                          \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | to_device                                          external                                          \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::TilizeWithValPadding                 device                                            \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::Transpose                            device                                            \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::Transpose                            device                                            \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::Unpad                                device                                            \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::Tilize                               device                                            \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::Tilize                               device                                            \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::Matmul                               device                                            \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::TilizeWithValPadding                 device                                            \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::EltwiseBinaryBroadcast               device                                            \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::Untilize                             device                                            \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | from_device                                        external                                          \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | to_torch                                           external                                          \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Tensor.reshape                                     external                                          \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | from_torch                                         external                                          \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | to_device                                          external                                          \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::TilizeWithValPadding                 device                                            \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::Transpose                            device                                            \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::Unpad                                device                                            \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::Matmul                               device                                            \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::DataTransferToDevice                 host                                              \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::EltwiseBinaryBroadcast               device                                            \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::TilizeWithValPadding                 device                                            \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::EltwiseBinaryBroadcast               device                                            \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::operations::primary::Softmax                   device                                            \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::Matmul                               device                                            \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::Pad                                  device                                            \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::Transpose                            device                                            \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::UntilizeWithUnpadding                device                                            \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::Tilize                               device                                            \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::Tilize                               device                                            \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::Matmul                               device                                            \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::TilizeWithValPadding                 device                                            \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::EltwiseBinaryBroadcast               device                                            \n"
     ]
    }
   ],
   "source": [
    "output = multi_head_attention(\n",
    "    hidden_states,\n",
    "    attention_mask,\n",
    "    query_weight,\n",
    "    query_bias,\n",
    "    key_weight,\n",
    "    key_bias,\n",
    "    value_weight,\n",
    "    value_bias,\n",
    "    output_weight,\n",
    "    output_bias,\n",
    "    head_size=head_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing ttnn tensor\n",
      "[1, 64, 128]\n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | tt::tt_metal::Untilize                             device                                            \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | from_device                                        external                                          \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | to_torch                                           external                                          \n",
      "Tensor([ [-0.0991211, 1.05469, -0.679688, -2.14062, 0.832031, 1.0625, 0.398438, 0.427734, 0.792969, -1.53906, -0.738281, 0.878906, -0.328125, 0.503906, 0.237305, -0.201172, -0.65625, 0.185547, -2.01562, 0.382812, -0.742188, 2.07812, 0.0245361, 0.878906, 1.14062, 0.345703, 2.20312, 1.25, 0.347656, 0.925781, 1.02344, -0.636719, 1.00781, 0.9375, 0.566406, 0.349609, -0.503906, 1.38281, 0.357422, 0.351562, -0.4375, 2.125, 0.398438, 2, 1.01562, 1.875, 0.292969, -1.33594, 1.92188, 1.07812, 0.386719, -1.26562, 1.65625, 0, -0.726562, 1.49219, -0.124512, 0.241211, -1.6875, -0.640625, 1.35156, 0, 0.558594, -1.6875, 0.339844, 0.0449219, 0.123047, -1.39062, -1.28906, -0.558594, 0.765625, -0.792969, -0.0761719, 0.357422, -1.78125, -0.195312, 2.57812, -1.45312, 0.539062, 1.375, -0.208008, 0.0291748, -2.0625, 2.07812, 1.46875, -0.882812, -0.523438, -2.25, 0.0673828, -0.777344, 1.30469, 1.17969, -0.0952148, 0.5, -0.605469, 0.392578, -0.376953, -1.9375, -1.86719, 0.710938, 0.214844, -0.742188, 1.76562, -0.296875, 0.53125, -1.57812, 0.326172, 1.84375, 2.09375, 0.554688, -0.96875, -0.201172, -0.0361328, -0.211914, -1.94531, -1.08594, 0.285156, 0.396484, -1.20312, 0.0839844, -1.45312, -0.851562, -1.10156, 0.390625, -1.24219, 0.851562, -0.265625, 0.0927734]], dtype=bfloat16 )\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Printing torch tensor\n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | _TensorBase.__getitem__                            external                                          \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | from_torch                                         external                                          \n",
      "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | to_torch                                           external                                          \n",
      "torch.Size([1, 64, 128])\n",
      "tensor([[-0.0991,  1.0547, -0.6797, -2.1406,  0.8320,  1.0625,  0.3984,  0.4277,\n",
      "          0.7930, -1.5391, -0.7383,  0.8789, -0.3281,  0.5039,  0.2373, -0.2012,\n",
      "         -0.6562,  0.1855, -2.0156,  0.3828, -0.7422,  2.0781,  0.0245,  0.8789,\n",
      "          1.1406,  0.3457,  2.2031,  1.2500,  0.3477,  0.9258,  1.0234, -0.6367,\n",
      "          1.0078,  0.9375,  0.5664,  0.3496, -0.5039,  1.3828,  0.3574,  0.3516,\n",
      "         -0.4375,  2.1250,  0.3984,  2.0000,  1.0156,  1.8750,  0.2930, -1.3359,\n",
      "          1.9219,  1.0781,  0.3867, -1.2656,  1.6562,  0.0000, -0.7266,  1.4922,\n",
      "         -0.1245,  0.2412, -1.6875, -0.6406,  1.3516,  0.0000,  0.5586, -1.6875,\n",
      "          0.3398,  0.0449,  0.1230, -1.3906, -1.2891, -0.5586,  0.7656, -0.7930,\n",
      "         -0.0762,  0.3574, -1.7812, -0.1953,  2.5781, -1.4531,  0.5391,  1.3750,\n",
      "         -0.2080,  0.0292, -2.0625,  2.0781,  1.4688, -0.8828, -0.5234, -2.2500,\n",
      "          0.0674, -0.7773,  1.3047,  1.1797, -0.0952,  0.5000, -0.6055,  0.3926,\n",
      "         -0.3770, -1.9375, -1.8672,  0.7109,  0.2148, -0.7422,  1.7656, -0.2969,\n",
      "          0.5312, -1.5781,  0.3262,  1.8438,  2.0938,  0.5547, -0.9688, -0.2012,\n",
      "         -0.0361, -0.2119, -1.9453, -1.0859,  0.2852,  0.3965, -1.2031,  0.0840,\n",
      "         -1.4531, -0.8516, -1.1016,  0.3906, -1.2422,  0.8516, -0.2656,  0.0928]],\n",
      "       dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "print(\"Printing ttnn tensor\")\n",
    "output = ttnn.to_layout(output, ttnn.ROW_MAJOR_LAYOUT)\n",
    "output = ttnn.from_device(output)\n",
    "print(output.shape)\n",
    "print(output[0, :1])\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"Printing torch tensor\")\n",
    "torch_output = ttnn.to_torch(output)\n",
    "print(torch_output.shape)\n",
    "print(torch_output[0, :1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Close the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Closing device 0\n"
     ]
    }
   ],
   "source": [
    "ttnn.close(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
