{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Multi-Head Attention\n",
                "\n",
                "Multi-Head Attention is an important part of all Transformer-based models.\n",
                "This tutorial will show how to write it and how to then optimize it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Initializing device 0\n",
                        "\u001b[38;2;000;128;000m                 Device\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Opening user mode device driver\n",
                        "\u001b[32m2023-12-05 03:37:48.916\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Detected 1 PCI device\n",
                        "\u001b[32m2023-12-05 03:37:48.927\u001b[0m | \u001b[1m\u001b[38;2;255;165;000mWARNING \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - init_detect_tt_device_numanodes(): Could not determine NumaNodeSet for TT device (physical_device_id: 0 pci_bus_id: 0000:00:08.0)\n",
                        "\u001b[32m2023-12-05 03:37:48.927\u001b[0m | \u001b[1m\u001b[38;2;255;165;000mWARNING \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Could not find NumaNodeSet for TT Device (physical_device_id: 0 pci_bus_id: 0000:00:08.0)\n",
                        "\u001b[32m2023-12-05 03:37:48.927\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Using 1 Hugepages/NumHostMemChannels for TTDevice (pci_interface_id: 0 device_id: 0xfaca revision: 0)\n",
                        "\u001b[32m2023-12-05 03:37:48.928\u001b[0m | \u001b[1m\u001b[38;2;255;165;000mWARNING \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - bind_area_memory_nodeset(): Unable to determine TT Device to NumaNode mapping for physical_device_id: 0. Skipping membind.\n",
                        "\u001b[0;33m---- ttSiliconDevice::init_hugepage: bind_area_to_memory_nodeset() failed (physical_device_id: 0 ch: 0). Hugepage allocation is not on NumaNode matching TT Device. Side-Effect is decreased Device->Host perf (Issue #893).\n",
                        "\u001b[0m\u001b[32m2023-12-05 03:37:49.023\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Disable PCIE DMA\n",
                        "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | AI CLK for device 0 is:   1202 MHz\n",
                        "\u001b[38;2;000;128;000m           BuildKernels\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Skip generating erisc binaries for grayskull\n"
                    ]
                }
            ],
            "source": [
                "import time\n",
                "import torch\n",
                "import ttnn\n",
                "\n",
                "torch.manual_seed(0)\n",
                "\n",
                "device_id = 0\n",
                "device = ttnn.open(device_id)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Enable program cache"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program Cache: enabled.\n"
                    ]
                }
            ],
            "source": [
                "ttnn.enable_program_cache()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Write Multi-Head Attention using ttnn\n",
                "\n",
                "Multi-head can be implemented in `torch` using just 6 operations:\n",
                "1. `torch.matmul`\n",
                "2. `torch.add`\n",
                "3. `torch.reshape`\n",
                "4. `torch.permute`\n",
                "5. `torch.mul`\n",
                "6. `torch.softmax`\n",
                "\n",
                "`ttnn` provides the exact same APIs to do that and therefore multi-head attention can be implemented as shown below:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "def multi_head_attention(\n",
                "    hidden_states,\n",
                "    query_weight,\n",
                "    query_bias,\n",
                "    key_weight,\n",
                "    key_bias,\n",
                "    value_weight,\n",
                "    value_bias,\n",
                "    output_weight,\n",
                "    output_bias,\n",
                "    *,\n",
                "    num_heads,\n",
                "):\n",
                "    batch_size, sequence_size, hidden_size = hidden_states.shape\n",
                "    head_size = hidden_size // num_heads\n",
                "\n",
                "    query = hidden_states @ query_weight\n",
                "    query = query + query_bias\n",
                "    query = ttnn.reshape(query, (batch_size, sequence_size, num_heads, head_size))\n",
                "    query = ttnn.permute(query, (0, 2, 1, 3))\n",
                "\n",
                "    key = hidden_states @ key_weight\n",
                "    key = key + key_bias\n",
                "    key = ttnn.reshape(key, (batch_size, sequence_size, num_heads, head_size))\n",
                "    key = ttnn.permute(key, (0, 2, 3, 1))\n",
                "\n",
                "    value = hidden_states @ value_weight\n",
                "    value = value + value_bias\n",
                "    value = ttnn.reshape(value, (batch_size, sequence_size, num_heads, head_size))\n",
                "    value = ttnn.permute(value, (0, 2, 1, 3))\n",
                "\n",
                "    attention_scores = query @ key\n",
                "    attention_scores = attention_scores * (1 / (head_size**0.5))\n",
                "    attention_probs = ttnn.softmax(attention_scores, dim=-1)\n",
                "\n",
                "    context_layer = attention_probs @ value\n",
                "    context_layer = ttnn.permute(context_layer, (0, 2, 1, 3))\n",
                "    context_layer = ttnn.reshape(context_layer, (batch_size, sequence_size, hidden_size))\n",
                "\n",
                "    self_output = context_layer @ output_weight\n",
                "    self_output = self_output + output_bias\n",
                "\n",
                "    return self_output"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now that the model is written, let's create input tensors to run it and test it"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "batch_size = 8\n",
                "sequence_size = 384\n",
                "num_heads = 16\n",
                "head_size = 64\n",
                "hidden_size = num_heads * head_size"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Initialize activations and weights using torch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "torch_hidden_states = torch.randn((batch_size, sequence_size, hidden_size), dtype=torch.bfloat16)\n",
                "torch_query_weight = torch.randn((hidden_size, hidden_size), dtype=torch.bfloat16)\n",
                "torch_query_bias = torch.randn((hidden_size,), dtype=torch.bfloat16)\n",
                "torch_key_weight = torch.randn((hidden_size, hidden_size), dtype=torch.bfloat16)\n",
                "torch_key_bias = torch.randn((hidden_size,), dtype=torch.bfloat16)\n",
                "torch_value_weight = torch.randn((hidden_size, hidden_size), dtype=torch.bfloat16)\n",
                "torch_value_bias = torch.randn((hidden_size,), dtype=torch.bfloat16)\n",
                "torch_output_weight = torch.randn((hidden_size, hidden_size), dtype=torch.bfloat16)\n",
                "torch_output_bias = torch.randn((hidden_size,), dtype=torch.bfloat16)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Convert activations and weights to ttnn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "hidden_states = ttnn.from_torch(torch_hidden_states)\n",
                "query_weight = ttnn.from_torch(torch_query_weight)\n",
                "query_bias = ttnn.from_torch(torch_query_bias)\n",
                "key_weight = ttnn.from_torch(torch_key_weight)\n",
                "key_bias = ttnn.from_torch(torch_key_bias)\n",
                "value_weight = ttnn.from_torch(torch_value_weight)\n",
                "value_bias = ttnn.from_torch(torch_value_bias)\n",
                "output_weight = ttnn.from_torch(torch_output_weight)\n",
                "output_bias = ttnn.from_torch(torch_output_bias)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Move activations and weights to device"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "hidden_states = ttnn.to_device(hidden_states, device)\n",
                "query_weight = ttnn.to_device(query_weight, device)\n",
                "query_bias = ttnn.to_device(query_bias, device, memory_config=ttnn.L1_MEMORY_CONFIG)\n",
                "key_weight = ttnn.to_device(key_weight, device)\n",
                "key_bias = ttnn.to_device(key_bias, device, memory_config=ttnn.L1_MEMORY_CONFIG)\n",
                "value_weight = ttnn.to_device(value_weight, device)\n",
                "value_bias = ttnn.to_device(value_bias, device, memory_config=ttnn.L1_MEMORY_CONFIG)\n",
                "output_weight = ttnn.to_device(output_weight, device)\n",
                "output_bias = ttnn.to_device(output_bias, device, memory_config=ttnn.L1_MEMORY_CONFIG)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Run the first iteration of Multi-Head Attention"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Tilize                               finished in     0.442773428 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Tilize                               finished in     0.283801031 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Matmul                               finished in     0.491539668 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::TilizeWithValPadding                 finished in     0.457688582 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::EltwiseBinaryBroadcast               finished in     0.448720675 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Untilize                             finished in     0.472414818 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::TilizeWithValPadding                 finished in     0.609298323 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Transpose                            finished in     0.499662371 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Unpad                                finished in     0.399145139 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Tilize                               finished in     0.005461198 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Tilize                               finished in     0.001622641 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Matmul                               finished in     0.001249763 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::TilizeWithValPadding                 finished in     0.000981905 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::EltwiseBinaryBroadcast               finished in     0.000999204 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Untilize                             finished in     0.006025776 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::TilizeWithValPadding                 finished in     0.159281236 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Transpose                            finished in     0.006215404 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Transpose                            finished in     0.419231764 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Unpad                                finished in     0.000587266 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Tilize                               finished in     0.005845266 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Tilize                               finished in     0.001785589 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Matmul                               finished in     0.001262353 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::TilizeWithValPadding                 finished in     0.000978485 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::EltwiseBinaryBroadcast               finished in     0.001028294 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Untilize                             finished in     0.006020425 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::TilizeWithValPadding                 finished in     0.159318685 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Transpose                            finished in     0.006208665 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Unpad                                finished in     0.000585587 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Matmul                               finished in     0.478780022 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::EltwiseBinaryBroadcast               finished in     0.448731994 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::operations::primary::Softmax                   finished in     0.655450458 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Matmul                               finished in     0.359853754 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Pad                                  finished in     0.474679195 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Transpose                            finished in      0.00877007 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::UntilizeWithUnpadding                finished in     0.495664095 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Tilize                               finished in     0.005473868 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Tilize                               finished in     0.001623341 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Matmul                               finished in     0.001245842 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::TilizeWithValPadding                 finished in     0.000983624 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::EltwiseBinaryBroadcast               finished in     0.001023464 seconds\n"
                    ]
                }
            ],
            "source": [
                "start = time.time()\n",
                "multi_head_attention(\n",
                "    hidden_states,\n",
                "    query_weight,\n",
                "    query_bias,\n",
                "    key_weight,\n",
                "    key_bias,\n",
                "    value_weight,\n",
                "    value_bias,\n",
                "    output_weight,\n",
                "    output_bias,\n",
                "    num_heads=num_heads,\n",
                ")\n",
                "end = time.time()\n",
                "duration = end - start"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Multi-head attention ran in 7.927839279174805 seconds for the first iteration\n"
                    ]
                }
            ],
            "source": [
                "print(f\"Multi-head attention ran in {duration} seconds for the first iteration\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Run a subsequent iteration of Multi-Head Attention"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Tilize                               finished in     0.005479948 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Tilize                               finished in     0.001622081 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Matmul                               finished in     0.001215313 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::TilizeWithValPadding                 finished in     0.000957504 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::EltwiseBinaryBroadcast               finished in     0.001032264 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Untilize                             finished in     0.006009236 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::TilizeWithValPadding                 finished in     0.159162096 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Transpose                            finished in     0.006242065 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Unpad                                finished in     0.000580037 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Tilize                               finished in     0.005395269 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Tilize                               finished in     0.001622991 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Matmul                               finished in     0.001225143 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::TilizeWithValPadding                 finished in     0.001023154 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::EltwiseBinaryBroadcast               finished in     0.001030734 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Untilize                             finished in     0.006006996 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::TilizeWithValPadding                 finished in     0.159072357 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Transpose                            finished in     0.006194785 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Transpose                            finished in     0.000918184 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Unpad                                finished in     0.000704116 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Tilize                               finished in     0.005804637 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Tilize                               finished in      0.00163913 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Matmul                               finished in     0.001253143 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::TilizeWithValPadding                 finished in     0.000942154 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::EltwiseBinaryBroadcast               finished in     0.001030634 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Untilize                             finished in     0.006018955 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::TilizeWithValPadding                 finished in     0.159161376 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Transpose                            finished in     0.006213805 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Unpad                                finished in     0.000608437 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Matmul                               finished in     0.003733099 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::EltwiseBinaryBroadcast               finished in     0.002043158 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::operations::primary::Softmax                   finished in     0.001929338 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Matmul                               finished in     0.002733584 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Pad                                  finished in     0.005485409 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Transpose                            finished in      0.00878526 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::UntilizeWithUnpadding                finished in     0.014012829 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Tilize                               finished in     0.005516848 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Tilize                               finished in      0.00162116 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Matmul                               finished in     0.001230143 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::TilizeWithValPadding                 finished in     0.000995075 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::EltwiseBinaryBroadcast               finished in     0.001020304 seconds\n"
                    ]
                }
            ],
            "source": [
                "start = time.time()\n",
                "output = multi_head_attention(\n",
                "    hidden_states,\n",
                "    query_weight,\n",
                "    query_bias,\n",
                "    key_weight,\n",
                "    key_bias,\n",
                "    value_weight,\n",
                "    value_bias,\n",
                "    output_weight,\n",
                "    output_bias,\n",
                "    num_heads=num_heads,\n",
                ")\n",
                "end = time.time()\n",
                "duration = end - start"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Multi-head attention ran in 0.6879308223724365 seconds for the subsequent iteration because of the program cache\n"
                    ]
                }
            ],
            "source": [
                "print(f\"Multi-head attention ran in {duration} seconds for the subsequent iteration because of the program cache\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Write optimized version of Multi-Head Attention\n",
                "\n",
                "Optimized version of the multi-head attention can be written by:\n",
                "- Tilizing all of the tensors ahead of time\n",
                "- Using more performant matmuls that fuse bias and specify the number of cores they execute on\n",
                "- Putting every tensor into L1\n",
                "- Using bfloat8_b data_type\n",
                "- Using custom `nlp` operations instead of `ttnn.permute` and `ttnn.reshape`\n",
                "\n",
                "`ttnn.deallocate` calls are needed because otherwise, the cores on the device will run out of the L1 memory"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "def optimized_multi_head_attention(\n",
                "    hidden_states,\n",
                "    fused_qkv_weight,\n",
                "    fused_qkv_bias,\n",
                "    self_output_weight,\n",
                "    self_output_bias,\n",
                "    *,\n",
                "    num_heads,\n",
                "    num_cores_x=12,\n",
                "):\n",
                "    batch_size, _, hidden_size = hidden_states.shape\n",
                "    head_size = hidden_size // num_heads\n",
                "    \n",
                "    hidden_states = ttnn.to_layout(hidden_states, ttnn.TILE_LAYOUT)\n",
                "\n",
                "    fused_qkv_output = ttnn.linear(\n",
                "        hidden_states,\n",
                "        fused_qkv_weight,\n",
                "        bias=fused_qkv_bias,\n",
                "        memory_config=ttnn.L1_MEMORY_CONFIG,\n",
                "        dtype=ttnn.bfloat8_b,\n",
                "        core_grid=(batch_size, num_cores_x),\n",
                "    )\n",
                "\n",
                "    (\n",
                "        query,\n",
                "        key,\n",
                "        value,\n",
                "    ) = ttnn.transformer.split_query_key_value_and_split_heads(\n",
                "        fused_qkv_output,\n",
                "        memory_config=ttnn.L1_MEMORY_CONFIG,\n",
                "        num_heads=num_heads,\n",
                "    )\n",
                "    ttnn.deallocate(fused_qkv_output)\n",
                "\n",
                "    attention_scores = ttnn.matmul(\n",
                "        query,\n",
                "        key,\n",
                "        memory_config=ttnn.L1_MEMORY_CONFIG,\n",
                "        dtype=ttnn.bfloat16,\n",
                "        core_grid=(batch_size, num_cores_x),\n",
                "    )\n",
                "    ttnn.deallocate(query)\n",
                "    ttnn.deallocate(key)\n",
                "\n",
                "    attention_probs = ttnn.transformer.attention_softmax(attention_scores, attention_mask=None, head_size=head_size)\n",
                "\n",
                "    context_layer = ttnn.matmul(\n",
                "        attention_probs,\n",
                "        value,\n",
                "        memory_config=ttnn.L1_MEMORY_CONFIG,\n",
                "        dtype=ttnn.bfloat8_b,\n",
                "        core_grid=(batch_size, num_cores_x),\n",
                "    )\n",
                "    ttnn.deallocate(attention_probs)\n",
                "\n",
                "    context_layer = ttnn.transformer.concatenate_heads(\n",
                "        context_layer,\n",
                "        memory_config=ttnn.L1_MEMORY_CONFIG,\n",
                "    )\n",
                "\n",
                "    self_output = ttnn.linear(\n",
                "        context_layer,\n",
                "        self_output_weight,\n",
                "        bias=self_output_bias,\n",
                "        memory_config=ttnn.L1_MEMORY_CONFIG,\n",
                "        dtype=ttnn.bfloat16,\n",
                "        core_grid=(batch_size, num_cores_x),\n",
                "    )\n",
                "    ttnn.deallocate(context_layer)\n",
                "\n",
                "    return self_output"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Pre-process the parameters of the optimized model\n",
                "\n",
                "1. Fuse QKV weights and biases\n",
                "2. Reshape and tilize for the optimized operations using preprocess_linear_weight and preprocess_linear_bias\n",
                "3. Move to device"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "from ttnn.model_preprocessing import (\n",
                "    preprocess_linear_bias,\n",
                "    preprocess_linear_weight,\n",
                ")\n",
                "\n",
                "torch_qkv_weight = torch.cat([torch_query_weight, torch_key_weight, torch_value_weight], dim=-1)\n",
                "torch_qkv_bias = torch.cat([torch_query_bias, torch_key_bias, torch_value_bias], dim=-1)\n",
                "\n",
                "qkv_weight = preprocess_linear_weight(torch_qkv_weight.T, dtype=ttnn.bfloat16)\n",
                "qkv_bias = preprocess_linear_bias(torch_qkv_bias, dtype=ttnn.bfloat16)\n",
                "output_weight = preprocess_linear_weight(torch_output_weight.T, dtype=ttnn.bfloat16)\n",
                "output_bias = preprocess_linear_bias(torch_output_bias, dtype=ttnn.bfloat16)\n",
                "\n",
                "qkv_weight = ttnn.to_device(qkv_weight, device)\n",
                "qkv_bias = ttnn.to_device(qkv_bias, device, memory_config=ttnn.L1_MEMORY_CONFIG)\n",
                "output_weight = ttnn.to_device(output_weight, device)\n",
                "output_bias = ttnn.to_device(output_bias, device, memory_config=ttnn.L1_MEMORY_CONFIG)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Run the first iteration of the optimized Multi-Head Attention"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Tilize                               finished in     0.005119581 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::operations::primary::Matmul                    finished in     0.650794504 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::operations::primary::transformers::SplitFusedQKVAndSplitHeads finished in     0.478532563 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::operations::primary::Matmul                    finished in     0.612131647 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::EltwiseBinaryBroadcast               finished in     0.385793305 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::operations::primary::Softmax                   finished in     0.001896549 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::operations::primary::Matmul                    finished in     0.591881792 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::NlpTM                                finished in      0.40587224 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::operations::primary::Matmul                    finished in     0.629241368 seconds\n"
                    ]
                }
            ],
            "source": [
                "start = time.time()\n",
                "hidden_states = ttnn.to_layout(hidden_states, ttnn.TILE_LAYOUT)\n",
                "optimized_output = optimized_multi_head_attention(\n",
                "    hidden_states,\n",
                "    qkv_weight,\n",
                "    qkv_bias,\n",
                "    output_weight,\n",
                "    output_bias,\n",
                "    num_heads=num_heads,\n",
                ")\n",
                "end = time.time()\n",
                "duration = end - start"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Optimized multi-head attention ran in 3.7684011459350586 seconds for the first iteration\n"
                    ]
                }
            ],
            "source": [
                "print(f\"Optimized multi-head attention ran in {duration} seconds for the first iteration\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Run a subsequent iteration of the optimized Multi-Head Attention"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::operations::primary::Matmul                    finished in     0.000593846 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::operations::primary::transformers::SplitFusedQKVAndSplitHeads finished in     0.000213378 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::operations::primary::Matmul                    finished in     0.000458387 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::EltwiseBinaryBroadcast               finished in     0.001472872 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::operations::primary::Softmax                   finished in     0.001904859 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::operations::primary::Matmul                    finished in     0.000856905 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::NlpTM                                finished in     0.000190919 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::operations::primary::Matmul                    finished in     0.000456968 seconds\n"
                    ]
                }
            ],
            "source": [
                "start = time.time()\n",
                "optimized_output = optimized_multi_head_attention(\n",
                "    hidden_states,\n",
                "    qkv_weight,\n",
                "    qkv_bias,\n",
                "    output_weight,\n",
                "    output_bias,\n",
                "    num_heads=num_heads,\n",
                ")\n",
                "end = time.time()\n",
                "duration = end - start"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Optimized multi-head attention ran in 0.008018732070922852 seconds for the subsequent iteration because of the program cache\n"
                    ]
                }
            ],
            "source": [
                "print(f\"Optimized multi-head attention ran in {duration} seconds for the subsequent iteration because of the program cache\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Note that the optimized multi-head attention is 2 orders of magnitude faster than the initial version"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Check that the output of the optimized version matches the output of the original implementation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Untilize                             finished in     0.006022145 seconds\n",
                        "\u001b[38;2;000;128;000m                     Op\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Program of Operation tt::tt_metal::Untilize                             finished in     0.367331572 seconds\n"
                    ]
                }
            ],
            "source": [
                "output = ttnn.to_layout(output, ttnn.ROW_MAJOR_LAYOUT)\n",
                "output = ttnn.from_device(output)\n",
                "torch_output = ttnn.to_torch(output)\n",
                "\n",
                "optimized_output = ttnn.to_layout(optimized_output, ttnn.ROW_MAJOR_LAYOUT)\n",
                "optimized_output = ttnn.from_device(optimized_output)\n",
                "torch_optimized_output = ttnn.to_torch(optimized_output)\n",
                "\n",
                "assert torch.allclose(torch_output, torch_optimized_output)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Close the device"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Closing device 0\n"
                    ]
                }
            ],
            "source": [
                "ttnn.close(device)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
